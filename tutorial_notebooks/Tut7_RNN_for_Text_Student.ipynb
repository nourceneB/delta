{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nourceneB/delta/blob/master/tutorial_notebooks/Tut7_RNN_for_Text_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f76703f",
      "metadata": {
        "id": "5f76703f"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/delta/blob/master/tutorial_notebooks/Tut7_RNN_for_Text_Student.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04aad623",
      "metadata": {
        "id": "04aad623"
      },
      "source": [
        "# Tutorial 7: RNNs for Text\n",
        "\n",
        "<br>\n",
        "<img src=\"https://i.imgur.com/pPPLHNn.png\" width=\"900\">\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d35170",
      "metadata": {
        "id": "41d35170"
      },
      "source": [
        "## Prediction of online review sentiment\n",
        "\n",
        "Sentiment analysis is one of the most popular applications for text classification. It is also interesting from a business perspective. For example, many companies have an interest in analyzing text data emerging in social media to understand how consumers value their products, service, brands, etc.\n",
        "\n",
        "The goal of sentiment analysis is to model the polarity of a piece of text, whether it is rather positive or rather negative. We can frame this task as a binary classification problem, with labels of one and zero indicating positive or negative sentiment, respectively. That is the approach we will take today. Other options  could involve modeling a three-level target (positive, neutral, negative) or a numeric target variable the values of which represent different strengths of polarity (e.g., between +5 and -5). Whenever approaching the sentiment analysis task by supervised learning, we depend on having some data with sentiment labels. That is often the real challenge in practice — where do the labels come from? — and explains why many labeled data sets re-occur in papers.\n",
        "\n",
        "We will apply different modeling approaches to predict review sentiment, from a simple dictionary-based approach over conventional supervised machine learning to several deep learning techniques. Here is the outline of the session.\n",
        "  \n",
        " 1. Preliminaries\n",
        " 2. Dictionary-based sentiment analysis\n",
        " 3. Text data representation\n",
        " 4. Deep learning based text classifiers<br>\n",
        "    - Feed-forward neural network <br>\n",
        "    - Recurrent neural network based on GRU<br>\n",
        "    - Bidirectional GRU<br>\n",
        "    - GRU with pre-trained IMDB embeddings<br>\n",
        "\n",
        "Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1209c1",
      "metadata": {
        "id": "2a1209c1"
      },
      "source": [
        "## 1. Preliminaries\n",
        "\n",
        "To set up our environment we load the IMDB 50K review data set and also the list of cleaned reviews. We then add the cleaned reviews to the data set to have everything in one place. It is a good idea to examine a few reviews and make sure that the original version and the cleaned version match. Ones this is confirmed, you can safely discard the raw review text to save memory. Finally, we update the coding of our target variable and encode positive and negative reviews as one and zero, respectively.  \n",
        "\n",
        "The notebook sets a new milestone in terms of demand for computational resources. We recommend running the notebook  in **Colab or another cloud-based platform of your choice**.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bd1cd5e2",
      "metadata": {
        "id": "bd1cd5e2"
      },
      "outputs": [],
      "source": [
        "# Import standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a12d0640",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a12d0640",
        "outputId": "59352db6-a499-4ac9-d5fe-1a8cf9070bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Create a global variable to idicate whether the notebook is run in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# Configure variables pointing to directories and stored files\n",
        "if IN_COLAB:\n",
        "    # Mount Google-Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATA_DIR = '/content/drive/My Drive/ProDok/'  # adjust to Google drive folder with the data if applicable\n",
        "else:\n",
        "    DATA_DIR = './' # adjust to the directory where data is stored on your machine (if running the notebook locally)\n",
        "\n",
        "sys.path.append(DATA_DIR)\n",
        "\n",
        "CLEAN_REVIEW = DATA_DIR + 'imdb_clean_full_v2.pkl'   # List with tokenized reviews after standard NLP preparation\n",
        "IMBD_EMBEDDINGS = DATA_DIR + 'w2v_imdb_full_d100_e500.model'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0344c6ff",
      "metadata": {
        "id": "0344c6ff"
      },
      "source": [
        "### Loading the data\n",
        "We load the data frame with the original and cleaned reviews. The original version does not matter for this session. We will delete them to save memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "220d7a80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "220d7a80",
        "outputId": "a766658d-fc7d-4ce3-9218-ee72afcaaf04"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/ProDok/imdb_clean_full_v2.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-970989980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLEAN_REVIEW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpath_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/ProDok/imdb_clean_full_v2.pkl'"
          ]
        }
      ],
      "source": [
        "with open(CLEAN_REVIEW,'rb') as path_name:\n",
        "    df = pd.read_pickle(path_name)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0815e98b",
      "metadata": {
        "id": "0815e98b"
      },
      "outputs": [],
      "source": [
        "# Drop original review text\n",
        "df.drop('review', axis=1, inplace=True)\n",
        "\n",
        "# Binary-encode the target variable\n",
        "df['sentiment'] = df['sentiment'].map({'positive' : 1, 'negative': 0})\n",
        "print(df['sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "846b8f9d",
      "metadata": {
        "id": "846b8f9d"
      },
      "source": [
        "### Downsampling the data to increase speed\n",
        "One more thing before moving on. You should decide whether you want to proceed with the full data frame (i.e., 50K reviews) or draw a random sample to decrease the runtime of the following examples. Using all the data is feasible on any descent computer but prepare for a bit of waiting when training our neural networks. Here is a little bit of code to reduce the amount of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77fc45a4",
      "metadata": {
        "id": "77fc45a4"
      },
      "outputs": [],
      "source": [
        "# Draw a random sample of n reviews to increase the speed of the following steps\n",
        "n = 5000\n",
        "np.random.seed(111)\n",
        "ix = np.random.randint(0, high=df.shape[0]-1, size=n)\n",
        "df = df.loc[ix, :]\n",
        "df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874f65f7",
      "metadata": {
        "id": "874f65f7"
      },
      "outputs": [],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a6f3b4",
      "metadata": {
        "id": "92a6f3b4"
      },
      "source": [
        "Before moving on with developing sentiment classification models, we partition our data into a training and a hold-out test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5d2fc9",
      "metadata": {
        "id": "8f5d2fc9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review_clean'], df['sentiment'], test_size=0.25, random_state=111)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a52b2f",
      "metadata": {
        "id": "57a52b2f"
      },
      "source": [
        "## 2. Dictionary-based sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a830529",
      "metadata": {
        "id": "8a830529"
      },
      "source": [
        "A simple approach to rate the sentiment of a text is to literally model it as the sum of its parts through the sentiment of each word. `pysentiment2` is a package containing several English word sentiment dictionaries. Word scores are either negative (-1), neutral (0) or positive (1). On a sentence level the polarity score is the average of positive and negative words (neutral words are ignored). The Harvard IV-4 dictionary, which we will use, consists of 3,642 coded words. Note that you will need to install the library before being able to run the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc7d66bd",
      "metadata": {
        "id": "dc7d66bd"
      },
      "outputs": [],
      "source": [
        "import pysentiment2 as ps\n",
        "dc = ps.HIV4() # import Harvard IV-4 dictionary\n",
        "\n",
        "def get_sentiment_score(text):\n",
        "    score = round(dc.get_score(dc.tokenize(text))['Polarity'], 2)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "947cb6d6",
      "metadata": {
        "id": "947cb6d6"
      },
      "source": [
        "We look up the sentiment score for each word in turn and sum up the sentiment values over words. Here are a few examples. Quite easy, isn't it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba2a5c6",
      "metadata": {
        "id": "6ba2a5c6"
      },
      "outputs": [],
      "source": [
        "#* Some examples how to rate texts. Larger values indicate stronger positive feelings\n",
        "examples = [\"What a marvelous evening, the weather is simply delightful. Wonderful!\",\n",
        "            \"I am devastated, the donuts are not what they once were – disgusting.\",\n",
        "            \"To be or not to be?\"\n",
        "           ]\n",
        "for txt in examples:\n",
        "    print(txt, \"\\t -> Sentiment score:\", get_sentiment_score(txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341b4fc4",
      "metadata": {
        "id": "341b4fc4"
      },
      "outputs": [],
      "source": [
        "get_sentiment_score('bad')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b0fc9a3",
      "metadata": {
        "id": "5b0fc9a3"
      },
      "source": [
        "### Sentiment score calculation  \n",
        "Since we already have a data frame, why not add the sentiment score of every review as a new column. This is a nice use case for the `.apply()` function that Pandas data frames support. We score the cleaned version of the review. If you fancy a little exercise, consider to also score the original review text and compare the differences between the two scores. You could then identify reviews where the sentiment scores differ substantially between the original and cleaned text. That might point to some issues in our data cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d22844e",
      "metadata": {
        "id": "3d22844e"
      },
      "outputs": [],
      "source": [
        "# Add the Afinn scores to our data frame\n",
        "# Caution: if you use the full data set of 50K reviews, the scoring will take a while.\n",
        "start = time.time()\n",
        "df['sentiment_score'] = df['review_clean'].apply(get_sentiment_score)\n",
        "end = time.time()\n",
        "\n",
        "print('Processed {} reviews in {:.0f} sec.'.format(df.shape[0], end-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7eb96fd",
      "metadata": {
        "id": "d7eb96fd"
      },
      "outputs": [],
      "source": [
        "df['sentiment_score'].hist();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b89d02",
      "metadata": {
        "id": "f8b89d02"
      },
      "outputs": [],
      "source": [
        "print(df['sentiment_score'].describe()) # overall rather positive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c981c93",
      "metadata": {
        "id": "6c981c93"
      },
      "source": [
        "### Sentiment classifier assessment\n",
        "We can treat the sentiment scores as class predictions. Applying a classification cut-off of zero, we posit that every review with a positive score is classified as positive, and negative otherwise. We can then examine the predictive accuracy of the dictionary-based classifier using standard performance measures for binary classification. Since we will do so a couple of times, we put the code in a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa26b2bc",
      "metadata": {
        "id": "aa26b2bc"
      },
      "outputs": [],
      "source": [
        "# Assess sentiment classification models\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "def assess_sentiment_classifier(ytest, yhat, cut_off=0.5, plot_roc=True):\n",
        "    \"\"\"\n",
        "        Helper function to assess a classification model in terms of the AUC\n",
        "        and classification accuracy. We calculate the latter by comapring\n",
        "        continuous (e.g., probabilistic) classifier predictions to a cut_off.\n",
        "        Cases where the prediction exceeds the cut-off are classified as\n",
        "        positive.\n",
        "    \"\"\"\n",
        "    # Calculate discrete class predictions\n",
        "    yhat_c = np.where(yhat>cut_off, 1, 0)\n",
        "\n",
        "    # Calculate classification accuracy and AUC\n",
        "    acc = accuracy_score(ytest, yhat_c)\n",
        "    auc = roc_auc_score(ytest, yhat)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cmat = confusion_matrix(ytest, yhat_c)\n",
        "\n",
        "    # ROC analysis\n",
        "    if plot_roc==True:\n",
        "        fpr, tpr, _ = roc_curve(ytest, yhat)\n",
        "        plt.plot(fpr,tpr, label=\"AUC={:.4}\".format(auc));\n",
        "        plt.plot([0, 1], [0, 1], \"r--\")\n",
        "        plt.ylabel('True positive rate')\n",
        "        plt.xlabel('False positive rate')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.show();\n",
        "\n",
        "    return(auc, acc, cmat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8535ae65",
      "metadata": {
        "id": "8535ae65"
      },
      "outputs": [],
      "source": [
        "# Assess lexicon-based sentiment classifier\n",
        "auc, acc, cmat = assess_sentiment_classifier(ytest=df['sentiment'], yhat=df['sentiment_score'], cut_off=0)\n",
        "print(\"Lexicon-based sentiment classifier:\\tAUC={:.4f}\\tAccuracy={:.4f}\".format(auc, acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93ce4e5",
      "metadata": {
        "id": "c93ce4e5"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cmat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684545ed",
      "metadata": {
        "id": "684545ed"
      },
      "source": [
        "The ROC curve looks good, whereas the confusion matrix suggests that our dictionary-based classifier is biased towards positive reviews. Note that your result might differ depending on which data you are using (all reviews, random sample). Finally, note that our performance estimates come from the whole data. That is ok since the lexicon-based approach does not involve training. Therefore, there is no risk of overfitting and no need for data partitioning.\n",
        "<br>Since we are about to create more models, it makes sense to create a container in which we store the results of different models. Good options are either a dictionary or a data set. In this notebook, we use the latter approach. To ensure comparability across different approaches, we will re-calculate the performance of the lexicon-based model for the observations of the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf331da5",
      "metadata": {
        "id": "bf331da5"
      },
      "outputs": [],
      "source": [
        "# Extract test set cases\n",
        "yhat = df.loc[X_test.index, 'sentiment_score']\n",
        "\n",
        "# Compute test set performance\n",
        "auc, acc, _ = assess_sentiment_classifier(y_test, yhat, cut_off=0, plot_roc=False)\n",
        "\n",
        "# We will create many more models. Let's put every model's score into a data frame\n",
        "df_scores = pd.DataFrame(index=['Acc', 'AUC'], columns=['Lexicon'], data=[acc, auc])\n",
        "print(df_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4b24f1",
      "metadata": {
        "id": "2b4b24f1"
      },
      "source": [
        "## 3. Text data representation\n",
        "For the next models, we need to represent the text data in a numeric format, which the learning algorithms can process. The specific form of the data depends on the type of learning algorithm. Traditional machine learning algorithms use some form of count vectorization, meaning that the words in a text are replaced by their number of occurrence in that text. This format is also called the **document-term matrix (DTM)**. A tabular data structure in which rows are documents (e.g., reviews in our case) and the columns are the unique words in the vocabulary. Given that a document is seen as a **bag of words**, BoW is just another popular name to refer to corresponding text representations. You can find many sources on the Internet such as [this one](https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/) to obtain a more comprehensive explanation of the BoW model if interested. Since our focus is deep learning, we will not go there.  However, let's note one important caveat of the BoW approach. The size of the vocabulary is typically very large. For example, according this [Wikipedia page](https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words), there are about 470K unique words in the English language. Making every unique word a feature in the DTM, our matrix will be high dimensional. Stemming can help a bit but **high-dimensionality** is one key disadvantage of representing data in form of a DTM. We will address this issue by developing a LASSO classifier that is robust toward dimensionality.\n",
        "<br>\n",
        "One advantage of word vectors over the BoW approach is that they avoid high dimensionality by embedding words in a lower dimensional space. Using word vectors, a document is represented as a **sequence of words** in which each word is represented by its embedding. This is the data structure that we use for our deep learning-based models.\n",
        "<br>\n",
        "In order to create the two text representations for traditional machine learning classifiers and deep learning classifiers, we use the class `Tokenizer` from the `Keras` text processing API. Libraries like `scikit-learn` and others provide similar functionality (see, e.g., [here](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)). When using `Keras`, our first task is to create a tokenizer object and 'train' it, a bit like with models, on our text data. To that end, we use the function `fit_on_texts`. Afterwards, we can use two functions for converting our text into a BoW or a sequence of words for machine and deep learning models, respectively. These functions go by the telling names `texts_to_matrix()` and `texts_to_sequences()`, respectively. We illustrate both of them in the following. But let's first build the vocabulary. We will need to decide how large our vocabulary should be. In practice, that can be a tough decision, the pros and cons being potentially higher accuracy when using many/all words in the text corpus versus faster computations when dropping some infrequent words. We just make a somewhat arbitrary choice in this notebook but remember that the **vocabulary size** is a meta-parameter that you will want to examine when working on a serious case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7713aaf",
      "metadata": {
        "id": "a7713aaf"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary using Keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "NUM_WORDS = 2500  # 2.5K is a rather small vocabulary. This choice is only to increase speed. Increase the setting and check whether you can raise performance\n",
        "\n",
        "# Create tokenizer object and build vocab from the training set\n",
        "tok = Tokenizer(NUM_WORDS, oov_token=1)  # We fit the tokenizer to the training set reviews. The test set might include\n",
        "tok.fit_on_texts(X_train)  # words that are not part of the training data. The argument oov_token ensures that such new words are mapped to the specified index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d191c2",
      "metadata": {
        "id": "59d191c2"
      },
      "source": [
        "The fitted tokenizer object exposes several methods that summarize what was learned and facilitates looking at the internal data from different perspectives. Here are a few examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b9895b",
      "metadata": {
        "id": "69b9895b"
      },
      "outputs": [],
      "source": [
        "# On how many documents did we train?\n",
        "print(tok.document_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c84977",
      "metadata": {
        "id": "f7c84977"
      },
      "outputs": [],
      "source": [
        "# How many unique words?\n",
        "len(tok.word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49dfd384",
      "metadata": {
        "id": "49dfd384"
      },
      "outputs": [],
      "source": [
        "# A dictionary of words storing in how many documents a word appeared\n",
        "word = 'film'\n",
        "n = tok.word_docs[word]\n",
        "print('The word <{}> appeared in {} reviews.'.format(word, n))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0c9915",
      "metadata": {
        "id": "7e0c9915"
      },
      "source": [
        "We embed words in a lower dimensional vector space and represent text as a sequence of embedding vectors. More specifically, the input to a deep network is a sequence of one-hot encoded words. The mapping of words to embedding vectors happens within the first layer of the network, the **embedding layer**. Few examples below. For now, the fact that the mapping is part of the network architecture means that our pre-processing consists of mapping the sequence of words in a review to one-hot-vectors. More precisely, we map the words to a **sequence of integer values**. That is more efficient. Knowing the size of our vocabulary, V, a one-hot-vector is a vector of V-1 zeros and a single value if one. We can encode that by an integer that tells us the index of the one in the one-hot vector. `Keras` provides this type of text preparation via the function `texts_to_sequences()`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c8cb111",
      "metadata": {
        "id": "0c8cb111"
      },
      "outputs": [],
      "source": [
        "# Convert training set reviews to sequences of integer values\n",
        "X_tr_int = tok.texts_to_sequences(X_train)\n",
        "print('Type of result is: ', type(X_tr_int))\n",
        "print('Length of our list: ', len(X_tr_int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54010ffa",
      "metadata": {
        "id": "54010ffa"
      },
      "outputs": [],
      "source": [
        "X_tr_int[0][:9]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fc5d05",
      "metadata": {
        "id": "73fc5d05"
      },
      "source": [
        "We can convert-back the integers to words using the methods of the fitted tokenizer as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fd1d0e4",
      "metadata": {
        "id": "8fd1d0e4"
      },
      "outputs": [],
      "source": [
        "demo = [tok.index_word[token] for token in X_tr_int[0][:9]]\n",
        "demo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fd2ab0",
      "metadata": {
        "id": "27fd2ab0"
      },
      "source": [
        "And again back to integers..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2270d83",
      "metadata": {
        "id": "c2270d83"
      },
      "outputs": [],
      "source": [
        "[tok.word_index[token] for token in demo]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a759ec8",
      "metadata": {
        "id": "2a759ec8"
      },
      "source": [
        "Two remarks are appropriate at this point. First,  `text_to_sequences()` also randomizes the data. Therefore, when looking at the first training example, it is again a different example (review) compared to the one that was in the first position after calling `text_to_matrix()`. So don't be confused to see another text. Second, the mapping from words to integers in `Keras` uses a *hashing function* and this function does not guarantee that words get mapped to unique integers. Collisions are possible. You can check the `Keras` documentations for details; simply search for *hashing trick*. It is not dramatically important at this point but when you play with codes like the above, mapping words to integers and than back to words, and back to integer, you might realize some odd behavior of different words getting mapped to the same integer. Show you ever observe such behavior, now you know it's coming from the hashing trick.\n",
        "\n",
        "The data is almost ready. However, the `Keras` layers that we will use later expect the input data to have a fixed, pre-defined shape. Our reviews differ substantially in length. So, the next task on our todo list is to pad the reviews and ensure a consistent sequence length. We could consider artificially making every review as long as the longest one. This way, we would not lose any data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a7bcdf",
      "metadata": {
        "id": "22a7bcdf"
      },
      "outputs": [],
      "source": [
        "# Determine the maximum review length in the training set\n",
        "max_review_length = max([len(review) for review in X_tr_int])\n",
        "print('The longest review of the training set has {} words.'.format(max_review_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1c187e8",
      "metadata": {
        "id": "f1c187e8"
      },
      "source": [
        "Standard practice in NLP is to embed words in a vector space. Considering an embedding dimension of, e.g., 100, each word in the input data (i.e., review) will be mapped to a 100 dim vector. Working with a large embedding dimension and long sequences will result in slow training. Since we care more about illustrating concepts than building the best possible sentiment classifier, we will set an upper bound on the text length and pad reviews accordingly. All reviews that are shorter than our upper bound will be padded with zeros. Longer reviews will be truncated. In practice, you would need to experiment carefully whether and how much truncating the data hurts performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21945653",
      "metadata": {
        "id": "21945653"
      },
      "outputs": [],
      "source": [
        "# Upper bound of the review length for padding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_REVIEW_LENGTH = 400\n",
        "\n",
        "X_tr_int_pad = pad_sequences(X_tr_int, MAX_REVIEW_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8beaaac",
      "metadata": {
        "id": "f8beaaac"
      },
      "outputs": [],
      "source": [
        "X_tr_int_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d6df2ff",
      "metadata": {
        "id": "3d6df2ff"
      },
      "source": [
        "So far, we dealt only with the training data. So it is about time to also process the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc45b6cc",
      "metadata": {
        "id": "bc45b6cc"
      },
      "outputs": [],
      "source": [
        "# Encode and pad the test data\n",
        "X_ts_int = tok.texts_to_sequences(X_test)  # Due to oov_token argument, new words will be mapped to 1\n",
        "X_ts_int_pad = pad_sequences(X_ts_int, MAX_REVIEW_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaeac05",
      "metadata": {
        "id": "adaeac05"
      },
      "outputs": [],
      "source": [
        "# Structure of the prepared training and test data\n",
        "X_tr_int_pad.shape, y_train.shape, X_ts_int_pad.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce71a77",
      "metadata": {
        "id": "3ce71a77"
      },
      "source": [
        "## 4. Deep learning-based text classifiers\n",
        "If we can use LASSO, we can also use a neural network for sentiment prediction. Session P.II.1 provided us with some flavor developing deep networks in `Keras`. The following part will provide many more examples. They will also introduce us to using recurrent and bidirectional architectures. Luckily, `Keras` makes using these fairly sophisticated neural networks relatively easy. Since our data is already of the right form, that is a sequence of integer, we can start right away and without further preparation with building networks.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d388b3f8",
      "metadata": {
        "id": "d388b3f8"
      },
      "source": [
        "#### Embedding layers\n",
        "When it comes to textual data, a crucial part of the architecture of a neural network is the **embedding layer**. `Keras` embedding layers support two use cases:\n",
        "* Learning an embedding as part of network training\n",
        "* Using pre-trained embeddings\n",
        "\n",
        "We will illustrate both use cases in the reminder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141b2e23",
      "metadata": {
        "id": "141b2e23"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GRU, Flatten, Dropout, LSTM, Bidirectional\n",
        "from keras.initializers import Constant"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df691d6",
      "metadata": {
        "id": "1df691d6"
      },
      "source": [
        "An Embedding is the entry point (i.e., first hidden layer) to a NN for text processing. When creating an embedding layer, you have to specify three arguments:\n",
        "\n",
        "* input_dim: This is the size of our vocabulary\n",
        "* output_dim: This is the embedding dimension, or, put differently the size of our word vectors\n",
        "* input_length: This is the length of our input sequences, i.e., the length of the reviews in words\n",
        "\n",
        "Knowing that these inputs are crucial further clarifies some previous steps, for example, why we had using padding to ensure a consistent length across all reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ea25dd",
      "metadata": {
        "id": "64ea25dd"
      },
      "outputs": [],
      "source": [
        "# Some variables to centralize the configuration of deep learning models\n",
        "NB_HIDDEN = 16        # Hidden nodes / state in fedforward / recurrent NNs\n",
        "EPOCH = 5             # Passes through the entire data set\n",
        "BATCH_SIZE = 64 #128  # Batch size\n",
        "EMBEDDING_DIM = 50    # Embedding dimension\n",
        "VAL_SPLIT = 0.25      # fraction of the training set used for validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05e72cb",
      "metadata": {
        "id": "f05e72cb"
      },
      "source": [
        "With these settings, we can create our embedding layer as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0692f38",
      "metadata": {
        "id": "b0692f38"
      },
      "outputs": [],
      "source": [
        "# Create an embedding layer\n",
        "emb_layer = Embedding(input_dim=NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_REVIEW_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "141be163",
      "metadata": {
        "id": "141be163"
      },
      "source": [
        "#### Model 1: Feed-forward neural network\n",
        "\n",
        "Once we have an embedding layer, we can move and stack other layers on top of it. Let's first illustrate that approach with a simple feed-forward network. Importantly, the output of the embedding layer will be a matrix: one vector of size equal to the embedding dimension for each word in the input sequence. If we add a fully-connected (**dense**) layer on top of that, we need to **flatten** the output. If the length of a review is 6 and the embedding dimension is 8, then flattening means that we create a 6x8=42 dimension vector, which represents the input to the dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f5fd6d",
      "metadata": {
        "id": "92f5fd6d"
      },
      "outputs": [],
      "source": [
        "# Feed-forward neural network\n",
        "fnn = Sequential()\n",
        "fnn.add(emb_layer)                           # embedding layer\n",
        "fnn.add(Flatten())                           # flattening layer (to ensure compliance with the input of the next layer)\n",
        "fnn.add(Dense(NB_HIDDEN, activation='relu')) # fully-connected layer\n",
        "fnn.add(Dropout(0.2))                        # dropout layer to prevent overfitting\n",
        "fnn.add(Dense(1, activation='sigmoid'))      # output layer (one unit with sigmoid activation of binary classification)\n",
        "\n",
        "# compile the model\n",
        "fnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(fnn.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70641734",
      "metadata": {
        "id": "70641734"
      },
      "outputs": [],
      "source": [
        "# Just for fun, let's recalculate the number of parameters\n",
        "print('Embedding layer parameters: {}'.format(NUM_WORDS * EMBEDDING_DIM))\n",
        "print('Flatten layer output shape: {}'.format(MAX_REVIEW_LENGTH * EMBEDDING_DIM))\n",
        "print('Dense layer parameters: {}'.format(NB_HIDDEN * (MAX_REVIEW_LENGTH * EMBEDDING_DIM)+ NB_HIDDEN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b89f71",
      "metadata": {
        "id": "c9b89f71"
      },
      "outputs": [],
      "source": [
        "# fitting the model\n",
        "story = fnn.fit(X_tr_int_pad, y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_split=VAL_SPLIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e0f8616",
      "metadata": {
        "id": "4e0f8616"
      },
      "source": [
        "##### A little bit of infrastructure\n",
        "The FNN was the first model in a chain of models of increasing sophistication. Sounds promising doesn't it.\n",
        "Since we are about to train more and more networks, we should develop a little bit of infrastructure code to work with them. Specifically, for each network, we need to produce test set predictions. Also, we would like to examine the development of the loss during training; e.g., to judge whether increasing the number of epochs would make sense. Last, it would be useful to save trained models to disk. After all, we spent quite some time on training them to making a backup in case something goes wrong with out notebook makes a lot of sense. Let's develop some helpers for these tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a31e39b",
      "metadata": {
        "id": "8a31e39b"
      },
      "outputs": [],
      "source": [
        "def diag_nn(model, story, x_ts, y_ts, plot_roc=True, plot_loss=True):\n",
        "    '''\n",
        "        Diagnose fitted keras models by plotting results from the\n",
        "        story (e.g., development of training loss) and calculating\n",
        "        classification performance (accuracy & AUC) on the test set\n",
        "    '''\n",
        "\n",
        "    # Calculate test set predictions\n",
        "    yhat = model.predict(X_ts_int_pad)\n",
        "\n",
        "    # Calling our helper for classifier evaluation\n",
        "    auc, acc, cmat = assess_sentiment_classifier(y_ts, yhat, cut_off=0.5, plot_roc=plot_roc)\n",
        "    print(\"NN test set performance:\\tAUC={:.4f}\\tAccuracy={:.4f}\".format(auc, acc))\n",
        "    print('Confusion matrix:')\n",
        "    print(cmat)\n",
        "\n",
        "    if plot_loss:\n",
        "        plt.plot(story.history['loss'])\n",
        "        plt.plot(story.history['val_loss'])\n",
        "        plt.title('loss evolution')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'validation'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    return (auc, acc, cmat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e07f98e1",
      "metadata": {
        "id": "e07f98e1"
      },
      "outputs": [],
      "source": [
        "# Assess the performance of the FFN\n",
        "auc, acc, _ = diag_nn(fnn, story, X_ts_int_pad, y_test, plot_roc=True, plot_loss=True)\n",
        "\n",
        "# Add results to our data frame to keep track of results\n",
        "df_scores['FFN'] = [acc, auc]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4446cb59",
      "metadata": {
        "id": "4446cb59"
      },
      "source": [
        "#### Model 2: Recurrent neural network based on GRU\n",
        "If you felt that the lecture part on advanced RNNs with gated cells was a little complicated you will be pleased to see that using corresponding models is actually quite easy. <br>\n",
        "**Disclaimer** We do not promise good results to be easily obtained, but getting a GRU to work is not difficult. In fact, it is easy because we already have almost all we need in place. The following codes is almost identical with that of developing the FNN.\n",
        "<br>\n",
        "The only difference is that we use different layers to design a different type of NN, namely a GRU layer. We chose GRU over LSTM because training the former is faster. You can easily adjust the codes to build a LSTM. Simply exchange the type of RNN layer to LSTM. Done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56def9a6",
      "metadata": {
        "id": "56def9a6"
      },
      "outputs": [],
      "source": [
        "# GRU text classifier\n",
        "gru=sequential()\n",
        "gru.add(emb_layer)\n",
        "gru.add(GRU(NB_HIDDEN))\n",
        "gru.add(Dropout(0.2))\n",
        "gru.add(Dense(1,activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "print(gru.summary())"
      ],
      "metadata": {
        "id": "OuUkMymw8PpY"
      },
      "id": "OuUkMymw8PpY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda5e064",
      "metadata": {
        "id": "bda5e064"
      },
      "outputs": [],
      "source": [
        "# Assessing the first GRU\n",
        "auc, acc, _ = diag_nn(gru, story, X_ts_int_pad, y_test, plot_roc=True, plot_loss=True)\n",
        "\n",
        "# Add results to our data frame to keep track of results\n",
        "df_scores['GRU'] = [acc, auc]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f847a88",
      "metadata": {
        "id": "0f847a88"
      },
      "source": [
        "#### Model 3: Bidirectional Recurrent neural network based on GRU\n",
        "\n",
        "Bidirectional Recurrent Neural Networks (BiRNNs) are an extension of standard RNNs that process input sequences in both forward and backward directions. This dual context allows the network to capture dependencies from both past and future tokens in a sentence, which is particularly beneficial in sentiment analysis where meaning often depends on the full context.\n",
        "\n",
        "By incorporating both directions, BiRNNs improve contextual understanding and often lead to better performance on natural language tasks.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://i.imgur.com/iweqX2v.png\" width=\"900\">\n",
        "<br>\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c03fd2c",
      "metadata": {
        "id": "1c03fd2c"
      },
      "outputs": [],
      "source": [
        "# Bidirectional GRU text classifier\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "173d1e48",
      "metadata": {
        "id": "173d1e48"
      },
      "outputs": [],
      "source": [
        "# Assessing the Bidirectional GRU\n",
        "auc, acc, _ = diag_nn(bi_gru, story, X_ts_int_pad, y_test, plot_roc=True, plot_loss=True)\n",
        "\n",
        "# Add results to our data frame to keep track of results\n",
        "df_scores['Bi-GRU'] = [acc, auc]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1c1b62d",
      "metadata": {
        "id": "d1c1b62d"
      },
      "source": [
        "#### Model 4: LSTM with pre-trained IMDB embeddings\n",
        "\n",
        "The previous models incorporated an embedding layer. However, training the weights of these layers, that is learning the word embeddings, was part of training the network. You can imagine that embeddings that you learn on-the-fly while training are different from embeddings that result from a model that is specifically designed to learn embeddings such as Word-to-Vec. Weights in Model 1 and Model 2  including the weights of the embedding matrix were trained to predict review sentiment. Word-to-Vec, on the other hand, solves a different prediction task related to the co-occurrences of words in a pre-defined context window. We have trained corresponding weights in the previous coding session P.II.2. It is about time to put these embeddings into action. Our next model will be similar to the previous one but use our own pre-trained IMDB embeddings resulting from applying the `Gensim` implementation of word-to-vec to our data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c4ba98",
      "metadata": {
        "id": "14c4ba98"
      },
      "source": [
        "##### Loading pre-trained embeddings\n",
        "\n",
        "Let's start with loading the `Gensim` embeddings. A version is available in our shared folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40b5ed8b",
      "metadata": {
        "id": "40b5ed8b"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "w2v = KeyedVectors.load_word2vec_format(IMBD_EMBEDDINGS, binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4f6c81",
      "metadata": {
        "id": "9e4f6c81"
      },
      "outputs": [],
      "source": [
        "# Load pretrained W2V embeddings obtained from the IMDB review data set\n",
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "\n",
        "imdb_index = KeyedVectors.load_word2vec_format(IMBD_EMBEDDINGS, binary=False)\n",
        "\n",
        "print('Loaded pre-trained embeddings for {} words in {:.0f} sec.'.format(len(imdb_index.key_to_index), time.time()-start))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be12e37",
      "metadata": {
        "id": "7be12e37"
      },
      "source": [
        "Pre-trained embeddings are essentially just a bunch of numbers. Every word has its own bunch of numbers or, more formally, numeric representation. Needless to say, the numbers are supposed to carry meaning, capturing syntactic and semantic relationships between words, etc. The embeddings that we just loaded come in the form of a dictionary. In the dictionary, words serve as the key and the corresponding value is the pre-trained embedding of that word. Let's illustrate this using the word *movie* as an example.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a2127c",
      "metadata": {
        "id": "a0a2127c"
      },
      "outputs": [],
      "source": [
        "imdb_index['movie']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ef4048",
      "metadata": {
        "id": "d4ef4048"
      },
      "source": [
        "Having loaded this type of dictionary, our next task is to look-up the embeddings for each of the words in our vocabulary. In simpler terms, for each word that we encounter in our text, we need to look-up its embedding. We will face the same task when working with Glove embeddings later. So let's wrap-up the code in a helper function, which we can reuse later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59c1028",
      "metadata": {
        "id": "e59c1028"
      },
      "outputs": [],
      "source": [
        "def get_embedding_matrix(tokenizer, pretrain, vocab_size, verbose=0):\n",
        "    '''\n",
        "        Helper function to construct an embedding matrix for\n",
        "        the focal corpus based on some pre-trained embeddings.\n",
        "    '''\n",
        "\n",
        "    dim = 0\n",
        "    # We will use the function with different types of embeddings. Therefore,\n",
        "    # we need a condition to determine what is the right way of determining\n",
        "    # the embedding dimension.\n",
        "    if isinstance(pretrain, KeyedVectors) or isinstance(pretrain, Word2VecKeyedVectors):\n",
        "        dim = pretrain.vector_size\n",
        "    elif isinstance(pretrain, dict):\n",
        "        dim = next(iter(pretrain.values())).shape[0]  # get embedding of an arbitrary word\n",
        "    else:\n",
        "        raise Exception('{} is not supported'.format(type(pretrain)))\n",
        "\n",
        "\n",
        "    # Initialize embedding matrix\n",
        "    emb_mat = np.zeros((vocab_size, dim))\n",
        "\n",
        "    # There will be some words in our corpus for which we lack a pre-trained embedding.\n",
        "    # In this tutorial, we will simply use a vector of zeros for such words. We also keep\n",
        "    # track of the words to do some debugging if needed\n",
        "    oov_words = []\n",
        "    # Below we use the tokenizer object that created our task vocabulary. This is crucial to ensure\n",
        "    # that the position of a words in our embedding matrix corresponds to its index in our integer\n",
        "    # encoded input data\n",
        "    v = len(tokenizer.word_index)\n",
        "    start = time.time()\n",
        "    print('Start embedding process for {} words.'.format(v), flush=True)\n",
        "\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        # try-catch together with a zero-initilaized embedding matrix achieves our rough fix for oov words\n",
        "        try:\n",
        "            emb_mat[i] = pretrain[word]\n",
        "        except:\n",
        "            oov_words.append(word)\n",
        "        # Some output that the method is still alive\n",
        "        if i % 5000 == 0 and verbose>0:\n",
        "            print('{}/{} words in {} sec'.format(i, v, (time.time()-start)), flush=True)\n",
        "\n",
        "\n",
        "    print('Created embedding matrix of shape {} in {} min '.format(emb_mat.shape, (time.time()-start)/60))\n",
        "\n",
        "    print('Encountered {} out-of-vocabulary words.'.format(len(oov_words)))\n",
        "    return (emb_mat, oov_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec7c160",
      "metadata": {
        "id": "7ec7c160"
      },
      "source": [
        "A few more words on embeddings...\n",
        "\n",
        "At present, we use embeddings that were obtained from the same corpus, namely the IMDB movie review data set, as the one we are working with right now. That is not common. Typically, the pre-training was done on some other — much larger — corpora. Remember that the very purpose of using pre-trained embeddings is that we hope the pre-trained embeddings to embody some information about word relationships that also prove valuable for our task. The larger the pre-trainind corpus the better. That said, there are some pitfalls that we need to be aware of. For example, when working with two different corpora, that used for pre-training embeddings and that used in the target task, the pre-training corpus will include words that do not appear in our corpus. That is less of a problem. More important, however, our corpus might — and often will — include some words for which we lack an embedding. Addressing this issue in a satisfactory manner is out of the scope of this tutorial. Pre-training an embedding for unknown words is a way forward but not a silver bullet. We will simply apply a rough fix and map out-of-vocabulary words to an embedding vector of zeros. Last, and most importantly, a matrix of pre-trained embeddings functions like a lookup table. The Keras embedding layer will not compute a dot product between a one-hot encoded input word and the embedding matrix because this would be inefficient. Instead, Keras expects to find the embedding of a word with index i in the i'th row of the embedding matrix. Therefore, **it is critical that index of a word in our embedding matrix is the same as in our dictionary of word indices**. Remember that we represent words as integers and sentences as a sequence of integers. Assuming a fictious encoded sentence [2, 44, 21], it would be important that the embedding matrix stores the word vectors of the words, which were mapped to the numbers 2, 44, and 21, respectively, in row 2, 44, and 21, respectively. Make sure you convince yourself that our above helper function achieves this consistency.\n",
        "\n",
        "Having understood all these details, we can eventually create our embedding matrix using our helper function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240c10b9",
      "metadata": {
        "id": "240c10b9"
      },
      "outputs": [],
      "source": [
        "# Create embedding weight matrix\n",
        "imdb_embeddings, _ = get_embedding_matrix(tok, imdb_index, NUM_WORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e6abd8",
      "metadata": {
        "id": "09e6abd8"
      },
      "source": [
        "For illustration, let's have a look at the embedding of the word 'movie'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81afc98",
      "metadata": {
        "id": "e81afc98"
      },
      "outputs": [],
      "source": [
        "imdb_embeddings[tok.word_index['movie'],:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6767a330",
      "metadata": {
        "id": "6767a330"
      },
      "source": [
        "It's time to build another model, this time using our IMDB embeddings. Specifically, we illustrate how to **initialize the weights** in the `Keras` **embedding layer** with our **pre-trained embeddings**. This is the way in which you use a pre-trained embedding in `Keras`. No matter what type of pre-training embedding you obtained and from where, to use it, you set the embedding layer weights to your embedding matrix. This is why it is so critical to **pay attention to word indices**; just in case we have not mentiond it ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e3e5b3",
      "metadata": {
        "id": "b5e3e5b3"
      },
      "outputs": [],
      "source": [
        "# Create an embedding layer using pre-trained embeddings\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fc7ea8",
      "metadata": {
        "id": "05fc7ea8"
      },
      "outputs": [],
      "source": [
        "# Assessing the first GRU\n",
        "auc, acc, _ = diag_nn(gru_imdb, story, X_ts_int_pad, y_test, plot_roc=True, plot_loss=True)\n",
        "\n",
        "# Add results to our data frame to keep track of results\n",
        "df_scores['GRU_IMDB'] = [acc, auc]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc9e2ee7",
      "metadata": {
        "id": "cc9e2ee7"
      },
      "source": [
        "## Conclusions\n",
        "The examples have covered important concepts in deep learning for NLP and text classification. We developed several deep learning-based text classifiers using Keras and advanced our understanding of word embeddings. We also saw examples of how to use pre-trained word embeddings in downstream tasks, such as sentiment analysis. Advancing the code to approach other text classification systems is straightforward. On the other hand, the results of the Keras models are not yet impressive. Here is th overview of the performance over all classifiers developed in the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a6861e",
      "metadata": {
        "id": "a4a6861e"
      },
      "outputs": [],
      "source": [
        "print(df_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ca060ac",
      "metadata": {
        "id": "1ca060ac"
      },
      "source": [
        "Considering the performance of the linear model, which is very easy to develop, fast, and memory efficient, we have not yet seen evidence that we need deep learning for the focal sentiment analysis task. It would be a very good exercise and try tuning the more advanced models, starting with proper training for more epochs, to raise their performance and achieve at least competitiveness with the linear model, and maybe the FNN. Good luck ;)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".delta-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}